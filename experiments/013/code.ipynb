{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Kaggle: False\n",
      "Input directory: /workspace/data/input\n",
      "Output directory: /workspace/data/output/013/1\n",
      "Artifact directory: /workspace/data/output\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kaggle Submission Notebook for CSIRO Biomass Prediction\n",
    "========================================================\n",
    "This notebook runs inference using trained models and creates submission.csv\n",
    "\n",
    "Usage:\n",
    "1. Train models locally: python train.py --config configs/exp/exp001.yaml\n",
    "2. Upload model weights and code to Kaggle (run cell-2 locally)\n",
    "3. Upload this notebook to Kaggle with the data sources attached\n",
    "4. Run this notebook on Kaggle\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggle環境ではコードDatasetからインポートするためにsys.pathを設定\n",
    "# Dataset名: csiro-biomass-codes-{EXP_NAME} (例: csiro-biomass-codes-001)\n",
    "if os.getenv(\"KAGGLE_DATA_PROXY_TOKEN\"):\n",
    "    sys.path.insert(0, \"/kaggle/input/csiro-biomass-codes-013\")\n",
    "\n",
    "import config\n",
    "import pandas as pd\n",
    "\n",
    "# Check environment\n",
    "print(f\"Running in Kaggle: {config.IS_KAGGLE_ENV}\")\n",
    "print(f\"Input directory: {config.INPUT_DIR}\")\n",
    "print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"Artifact directory: {config.ARTIFACT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run inference (DINOv3)\n",
    "# from inference_dinov3 import kaggle_inference\n",
    "\n",
    "# # Configuration - Change run_name to match your best run\n",
    "# RUN_NAME = \"001_exp013_dinov3__lr-0_002__weight_decay-0_01\"\n",
    "# FOLDS = None  # None = use all 5 folds (0-4), or specify list like [0, 1, 2]\n",
    "# IMG_SIZE = 960  # Image size (must be divisible by 16)\n",
    "# WEIGHT_TYPE = \"best\"  # \"best\" (lowest loss) or \"last\" (final epoch)\n",
    "\n",
    "# # Run inference and create submission\n",
    "# # TTA (4 flip variants) is always enabled\n",
    "# submission_df = kaggle_inference(\n",
    "#     run_name=RUN_NAME,\n",
    "#     folds=FOLDS,\n",
    "#     img_size=IMG_SIZE,\n",
    "#     weight_type=WEIGHT_TYPE,\n",
    "# )\n",
    "\n",
    "# # Save submission\n",
    "# submission_df.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"Submission saved to submission.csv\")\n",
    "# print(f\"Submission shape: {submission_df.shape}\")\n",
    "# submission_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading experiment: /workspace/data/output/013/1/20251227_174406_exp013_dinov3\n",
      "Copied backbone.pth to /workspace/data/output/013/1/20251227_174406_exp013_dinov3/backbone.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model instance not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dst_dir=/tmp/tmpsza2clin\n",
      "tree\n",
      "├── 001_dinov3_head__lr-0_002__weight_decay-0_01\n",
      "│   ├── config.yaml\n",
      "│   ├── logs\n",
      "│   │   ├── metrics.csv\n",
      "│   │   └── train.log\n",
      "│   ├── plots\n",
      "│   │   └── training_curves.png\n",
      "│   ├── predictions.csv\n",
      "│   ├── submission.csv\n",
      "│   └── weights\n",
      "│       ├── best_fold0.pth\n",
      "│       ├── best_fold1.pth\n",
      "│       ├── best_fold2.pth\n",
      "│       ├── best_fold3.pth\n",
      "│       ├── best_fold4.pth\n",
      "│       ├── last_fold0.pth\n",
      "│       ├── last_fold1.pth\n",
      "│       ├── last_fold2.pth\n",
      "│       ├── last_fold3.pth\n",
      "│       └── last_fold4.pth\n",
      "├── 002_dinov3_head__lr-0_002__weight_decay-0_001\n",
      "│   ├── config.yaml\n",
      "│   ├── logs\n",
      "│   │   └── train.log\n",
      "│   ├── plots\n",
      "│   └── weights\n",
      "│       ├── best_fold0.pth\n",
      "│       ├── best_fold1.pth\n",
      "│       ├── best_fold2.pth\n",
      "│       ├── best_fold3.pth\n",
      "│       ├── best_fold4.pth\n",
      "│       ├── last_fold0.pth\n",
      "│       ├── last_fold1.pth\n",
      "│       ├── last_fold2.pth\n",
      "│       └── last_fold3.pth\n",
      "└── backbone.pth\n",
      "Starting upload for file 002_dinov3_head__lr-0_002__weight_decay-0_001.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92.0M/92.0M [00:05<00:00, 18.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: 002_dinov3_head__lr-0_002__weight_decay-0_001.zip (92MB)\n",
      "Starting upload for file 001_dinov3_head__lr-0_002__weight_decay-0_01.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102M/102M [00:05<00:00, 19.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: 001_dinov3_head__lr-0_002__weight_decay-0_01.zip (102MB)\n",
      "Starting upload for file backbone.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.13G/3.13G [02:09<00:00, 25.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: backbone.pth (3GB)\n",
      "dst_dir=/tmp/tmpjdmj1z6o/csiro-biomass-codes-013\n",
      "tree\n",
      "├── README.md\n",
      "├── analyze_errors.py\n",
      "├── code.ipynb\n",
      "├── config.py\n",
      "├── configs\n",
      "│   └── exp\n",
      "│       └── exp013_dinov3.yaml\n",
      "├── extract_features.py\n",
      "├── inference_dinov3.py\n",
      "├── src\n",
      "│   ├── __init__.py\n",
      "│   ├── backbone.py\n",
      "│   ├── data.py\n",
      "│   ├── feature_dataset.py\n",
      "│   ├── head_model.py\n",
      "│   ├── kaggle_utils\n",
      "│   │   ├── __init__.py\n",
      "│   │   └── customhub.py\n",
      "│   ├── loss_function.py\n",
      "│   ├── manifold_mixup.py\n",
      "│   ├── metric.py\n",
      "│   └── seed.py\n",
      "└── train_head.py\n",
      "Starting upload for file code.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.98k/8.98k [00:00<00:00, 11.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: code.ipynb (9KB)\n",
      "Starting upload for file inference_dinov3.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14.0k/14.0k [00:00<00:00, 17.6kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: inference_dinov3.py (14KB)\n",
      "Starting upload for file src.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12.7k/12.7k [00:00<00:00, 14.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: src.zip (13KB)\n",
      "Starting upload for file extract_features.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.53k/7.53k [00:00<00:00, 8.83kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: extract_features.py (8KB)\n",
      "Starting upload for file analyze_errors.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22.1k/22.1k [00:00<00:00, 28.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: analyze_errors.py (22KB)\n",
      "Starting upload for file README.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.84k/5.84k [00:00<00:00, 7.41kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: README.md (6KB)\n",
      "Starting upload for file configs.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 918/918 [00:00<00:00, 1.21kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: configs.zip (918B)\n",
      "Starting upload for file train_head.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.1k/29.1k [00:00<00:00, 34.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train_head.py (29KB)\n",
      "Starting upload for file config.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.88k/2.88k [00:00<00:00, 3.80kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.py (3KB)\n"
     ]
    }
   ],
   "source": [
    "# Upload artifacts to Kaggle (run this locally after training)\n",
    "# Uncomment and run this cell to upload trained models and code\n",
    "\n",
    "# ============================================================\n",
    "# アップロードする experiment_dir を指定\n",
    "# data/output/013/1/ 内のディレクトリを確認して指定してください\n",
    "# ============================================================\n",
    "EXPERIMENT_DIR = \"20251227_174406_exp013_dinov3\"  # ← ここを変更\n",
    "\n",
    "if not config.IS_KAGGLE_ENV:\n",
    "    import shutil\n",
    "\n",
    "    from src.kaggle_utils.customhub import dataset_upload, model_upload\n",
    "\n",
    "    experiment_path = config.OUTPUT_DIR / EXPERIMENT_DIR\n",
    "    if not experiment_path.exists():\n",
    "        raise FileNotFoundError(f\"Experiment directory not found: {experiment_path}\")\n",
    "\n",
    "    print(f\"Uploading experiment: {experiment_path}\")\n",
    "\n",
    "    # Copy backbone.pth to experiment directory for upload\n",
    "    # backbone.pth is saved at OUTPUT_DIR/backbone.pth during feature extraction\n",
    "    backbone_src = config.OUTPUT_DIR / \"backbone.pth\"\n",
    "    backbone_dst = experiment_path / \"backbone.pth\"\n",
    "    if backbone_src.exists() and not backbone_dst.exists():\n",
    "        shutil.copy2(backbone_src, backbone_dst)\n",
    "        print(f\"Copied backbone.pth to {backbone_dst}\")\n",
    "    elif backbone_dst.exists():\n",
    "        print(f\"backbone.pth already exists at {backbone_dst}\")\n",
    "    else:\n",
    "        print(f\"WARNING: backbone.pth not found at {backbone_src}\")\n",
    "\n",
    "    # モデル重みを Kaggle Model としてアップロード\n",
    "    # experiment_dir 内の全 run_name をアップロード\n",
    "    model_upload(\n",
    "        handle=config.ARTIFACTS_HANDLE,\n",
    "        local_model_dir=experiment_path,\n",
    "        update=False,  # Set to True to update existing model\n",
    "    )\n",
    "\n",
    "    # 実験コードを Kaggle Dataset としてアップロード\n",
    "    # experiments/013/ ディレクトリのみ（プロジェクト全体ではない）\n",
    "    dataset_upload(\n",
    "        handle=config.CODES_HANDLE,\n",
    "        local_dataset_dir=config.EXP_DIR,  # experiments/013/ のみ\n",
    "        update=True,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
