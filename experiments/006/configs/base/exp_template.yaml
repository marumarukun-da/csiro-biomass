# =============================================================================
# Base experiment template for biomass prediction
# =============================================================================
# This is the base configuration that all experiments inherit from.
# Override specific values in experiment configs (e.g., configs/exp/exp001.yaml)
#
# Architecture:
#   Default: Dual Input model (left-right split with shared backbone)
#   - Original image (2000x1000) is split into left/right halves (1000x1000 each)
#   - Both halves are processed by shared backbone
#   - Features are concatenated and passed to regression head
#   - TTA is applied to original image before splitting
#
# Grid Search:
#   Any parameter specified as a list will be swept (grid search).
#   Example: backbone: [model_a, model_b] and lr: [1e-3, 1e-4] = 4 runs
#
# Exceptions (NOT swept even if list):
#   - experiment.notes
#   - augmentation.normalize.*
#   - augmentation.train.*
#   - dataset.target_cols
#   - loss.weights
# =============================================================================

# Experiment settings
experiment:
  name: exp000
  seed: 42
  output_dir: null  # null = use default from config.py
  notes: ""

# Dataset settings
dataset:
  # Paths (set in experiment config)
  train_csv: null
  image_dir: null

  # Image settings
  img_size:
    - 224
  in_chans: 3

  # Target columns to predict
  target_cols:
    - Dry_Dead_g
    - Dry_Green_g
    - Dry_Clover_g

# Training settings
trainer:
  # Batch sizes
  train_batch_size: 32
  val_batch_size: 64
  gradient_accumulation_steps: 1  # Effective batch size = train_batch_size * gradient_accumulation_steps

  # Workers
  num_workers: 4

  # Training
  num_epochs: 30
  use_amp: true

  # Cross-validation (GroupKFold by site)
  n_folds: 5
  fold: null  # null = train all folds, or specify fold number (0-4)
  group_col: site  # GroupKFold grouping column (State + Sampling_Date)

# Optimization settings
optimization:
  lr: 1e-4
  weight_decay: 1e-4
  warmup_rate: 0.1

  # EMA (Exponential Moving Average)
  use_ema: true
  ema_decay: 0.995
  ema_start_ratio: 0.1

# Model settings
model:
  backbone: tf_efficientnetv2_b0.in1k
  pretrained: true
  num_outputs: 3
  dropout: 0.2
  hidden_size: 512

# Loss function settings
loss:
  beta: 1.0  # Smooth L1 beta parameter

  # Target weights (equal weights for component prediction)
  weights:
    Dry_Dead_g: 1.0
    Dry_Green_g: 1.0
    Dry_Clover_g: 1.0

# Augmentation settings
augmentation:
  # Normalization (ImageNet defaults)
  normalize:
    mean:
      - 0.485
      - 0.456
      - 0.406
    std:
      - 0.229
      - 0.224
      - 0.225

  # Training augmentations
  train:
    horizontal_flip:
      p: 0.5
    vertical_flip:
      p: 0.5
    random_brightness_contrast:
      brightness_limit: 0.2
      contrast_limit: 0.2
      p: 0.25
    coarse_dropout:
      num_holes_range:
        - 10
        - 20
      hole_height_range:
        - 20
        - 100
      hole_width_range:
        - 20
        - 100
      p: 0.2
    gaussian_blur:
      blur_limit:
        - 3
        - 7
      p: 0.3
    hue_saturation_value:
      hue_shift_limit: 10
      sat_shift_limit: 20
      val_shift_limit: 20
      p: 0.5
    random_gamma:
      gamma_limit:
        - 60
        - 140
      p: 0.5
    random_rotate90:
      p: 0.3

  # Mixup augmentation (applied at batch level during training)
  mixup:
    enabled: false  # Set to true to enable Mixup
    alpha: 0.4      # Beta distribution parameter (0.1-0.4 recommended for regression)
    disable_ratio: 0.2  # Disable Mixup in the last X% of training (e.g., 0.2 = last 20%)

  # Validation uses only Resize + Normalize (automatic)
