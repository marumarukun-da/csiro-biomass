# Experiment 001 実装ガイド

このドキュメントは `experiments/001` フォルダの実装内容と、Kaggle への提出手順を詳細に説明します。

---

## 目次

1. [概要](#概要)
2. [ファイル構成](#ファイル構成)
3. [モデルアーキテクチャ](#モデルアーキテクチャ)
4. [設定ファイル（YAML）](#設定ファイルyaml)
5. [トレーニング手順](#トレーニング手順)
6. [推論手順](#推論手順)
7. [Kaggle提出手順](#kaggle提出手順)
8. [グリッドサーチの仕組み](#グリッドサーチの仕組み)
9. [重要な実装の詳細](#重要な実装の詳細)
10. [トラブルシューティング](#トラブルシューティング)

---

## 概要

### コンペティション

- **名前**: CSIRO Biomass Prediction
- **タスク**: 牧草地の画像から5つのバイオマス指標を予測（回帰タスク）
- **評価指標**: Globally Weighted R² Score

### 予測対象

| ターゲット | 重み | 予測/導出 |
|-----------|------|----------|
| Dry_Total_g | 0.5 | 予測 |
| GDM_g | 0.2 | 予測 |
| Dry_Green_g | 0.1 | 予測 |
| Dry_Dead_g | 0.1 | 導出: `Dry_Total_g - GDM_g` |
| Dry_Clover_g | 0.1 | 導出: `GDM_g - Dry_Green_g` |

**重要**: モデルは3つの値のみを予測し、残り2つは数式で導出します。

### 設計方針

- pj_kenpin スタイルの YAML 駆動グリッドサーチ
- Kaggle 環境との互換性維持（IS_KAGGLE_ENV による自動パス切り替え）
- 5-fold CV + EMA + TTA による堅牢な予測

---

## ファイル構成

```
experiments/001/
├── config.py                    # パス設定、環境判定
├── train.py                     # トレーニングスクリプト（グリッドサーチ対応）
├── inference.py                 # 推論スクリプト（TTA・アンサンブル対応）
├── code.ipynb                   # Kaggle 提出用ノートブック
│
├── configs/
│   ├── base/
│   │   └── exp_template.yaml    # 基本設定テンプレート
│   └── exp/
│       └── exp001.yaml          # 実験設定（これを編集）
│
└── src/
    ├── __init__.py
    ├── seed.py                  # 再現性のためのシード設定
    ├── metric.py                # Weighted R² スコア計算
    ├── loss_function.py         # 損失関数（Weighted Smooth L1）
    ├── model.py                 # モデル定義（DualInputRegressionNet）
    └── data.py                  # データセット、DataLoader、Augmentation
```

---

## モデルアーキテクチャ

### DualInputRegressionNet

左右分割画像を入力とするSiamese-likeアーキテクチャです。

```
Original Image [1000, 2000, 3]
         │
    ┌────┴────┐
    │ Split   │
    ▼         ▼
Left Half   Right Half
[1000,1000] [1000,1000]
    │         │
    ▼         ▼
┌─────────────────────┐
│  Shared Backbone    │  (EfficientNetV2, ConvNeXt, Swin 等)
│  (timm pretrained)  │
└─────────────────────┘
    │         │
    ▼         ▼
┌─────────────────────┐
│  GeM Pooling        │  (Generalized Mean Pooling, 学習可能なp)
└─────────────────────┘
    │         │
    ▼         ▼
[num_feat]  [num_feat]
    │         │
    └────┬────┘
         │ Concatenate
         ▼
  [num_features * 2]
         │
┌─────────────────────┐
│  MLP Head           │
│  - Dropout          │
│  - Linear → 512     │
│  - GELU             │
│  - Dropout          │
│  - Linear → 3       │
└─────────────────────┘
         │
         ▼ [B, 3]
Output: [Dry_Total_g, GDM_g, Dry_Green_g]
```

### GeM Pooling

通常の Average Pooling より柔軟なプーリング手法：

```python
GeM(x) = (1/|X| * Σ x^p)^(1/p)
```

- `p=1`: Average Pooling
- `p→∞`: Max Pooling
- `p` は学習可能パラメータ（デフォルト: 3.0）

### 推奨バックボーン

timm ライブラリで利用可能なバックボーンを使用できます。

```python
# EfficientNet V2 (速度と精度のバランスが良い)
"tf_efficientnetv2_b0.in1k"   # 速度重視（ベースライン向け）
"tf_efficientnetv2_b1.in1k"   # 速度重視
"tf_efficientnetv2_b2.in1k"   # バランス型
"tf_efficientnetv2_s.in1k"    # バランス型
"tf_efficientnetv2_m.in1k"    # 精度重視

# ConvNeXt (モダンなCNNアーキテクチャ)
"convnext_tiny.fb_in1k"       # バランス型
"convnext_small.fb_in1k"      # 精度重視
"convnext_base.fb_in1k"       # 精度重視

# Swin Transformer
"swin_tiny_patch4_window7_224.ms_in1k"   # バランス型
"swin_small_patch4_window7_224.ms_in1k"  # 精度重視

# EfficientNet (オリジナル)
"tf_efficientnet_b0.ns_jft_in1k"  # 速度重視
"tf_efficientnet_b1.ns_jft_in1k"  # バランス型
"tf_efficientnet_b2.ns_jft_in1k"  # バランス型
```

---

## 設定ファイル（YAML）

### 構造

```yaml
# configs/exp/exp001.yaml

__include__:                    # 継承元（base template）
  - "../base/exp_template.yaml"

experiment:
  name: exp001
  seed: 42
  notes: "Baseline experiment"

dataset:
  train_csv: ./data/input/csiro-biomass/train.csv
  image_dir: ./data/input/csiro-biomass
  img_size:
    - 224                       # リスト = グリッドサーチ対象

trainer:
  train_batch_size: 32
  val_batch_size: 64
  num_workers: 4
  num_epochs: 10
  use_amp: true
  n_folds: 5
  fold: null                    # null = 全fold、数値 = 指定foldのみ

optimization:
  lr:
    - 1e-3                      # リスト = グリッドサーチ対象
    - 1e-4
  weight_decay: 1e-4
  warmup_rate: 0.1
  use_ema: true
  ema_decay: 0.995

model:
  backbone:
    - tf_efficientnetv2_b0.in1k  # リスト = グリッドサーチ対象
    - tf_efficientnetv2_b1.in1k
  pretrained: true
  num_outputs: 3
  dropout: 0.2
  hidden_size: 512

loss:
  beta: 1.0                      # Smooth L1 の beta パラメータ
  weights:                       # ターゲット重み（グリッドサーチ対象外）
    Dry_Total_g: 0.5
    GDM_g: 0.2
    Dry_Green_g: 0.1

augmentation:
  normalize:
    mean: [0.485, 0.456, 0.406]  # ImageNet (グリッドサーチ対象外)
    std: [0.229, 0.224, 0.225]
  train:
    horizontal_flip:
      p: 0.5
    vertical_flip:
      p: 0.5
```

### グリッドサーチ対象外のパス

以下のパスはリストでもグリッドサーチの対象になりません：

```python
NON_SWEEP_PATHS = {
    ("experiment", "notes"),
    ("dataset", "target_cols"),
    ("loss", "weights"),
    ("augmentation", "normalize", "mean"),
    ("augmentation", "normalize", "std"),
}
```

---

## トレーニング手順

### 1. 環境準備

```bash
cd /path/to/csiro-biomass

# 必要なライブラリ（主要なもの）
# pip install torch torchvision timm albumentations transformers omegaconf scikit-learn tqdm matplotlib
```

### 2. トレーニング実行

```bash
cd experiments/001

# 基本実行
python train.py --config configs/exp/exp001.yaml
```

### 3. 出力構造

```
output/
└── {timestamp}_{experiment_name}/
    ├── summary.csv              # 全 run の結果一覧（R²降順）
    ├── summary.json             # JSON 形式のサマリー
    │
    ├── 001_tf_efficientnetv2_b0_in1k__lr-0_001/
    │   ├── config.yaml          # この run の設定
    │   ├── weights/
    │   │   ├── best_fold0.pth   # 各 fold のベストモデル
    │   │   ├── best_fold1.pth
    │   │   ├── ...
    │   │   └── last_fold4.pth
    │   ├── logs/
    │   │   ├── train.log        # トレーニングログ
    │   │   └── metrics.csv      # fold 別メトリクス
    │   └── plots/
    │       └── training_curves.png
    │
    ├── 002_tf_efficientnetv2_b0_in1k__lr-0_0001/
    │   └── ...
    └── ...
```

### 4. 結果の確認

```python
import pandas as pd

# summary.csv を確認
df = pd.read_csv("output/{timestamp}_exp001/summary.csv")
print(df[["run_name", "avg_val_r2", "model/backbone", "optimization/lr"]])
```

---

## 推論手順

### ローカルでの推論

```bash
cd experiments/001

# 基本実行
python inference.py --run_id run_0001 --folds all

# オプション
python inference.py \
    --run_id run_0001 \
    --folds 1,2,3 \      # 特定の fold のみ使用
    --use_split \         # 左右分割 TTA を有効化
    --no_tta \            # TTA を無効化
    --no_ema \            # EMA 重みを使わない
    --img_size 224 \
    --device cuda
```

### 出力ファイル

```
output/
├── submission.csv    # Kaggle 提出用（Long format）
└── predictions.csv   # 分析用（Wide format, 3列）
```

---

## Kaggle提出手順

### ワークフロー図

```
┌─────────────────────────────────────────────────────────────────┐
│                         ローカル環境                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. トレーニング実行                                              │
│     python train.py --config configs/exp/exp001.yaml            │
│                          │                                      │
│                          ▼                                      │
│     output/{timestamp}_exp001/                                   │
│     ├── 001_xxx/weights/best_fold*.pth                          │
│     └── summary.csv                                              │
│                          │                                      │
│  2. ベストモデルを選択                                            │
│     summary.csv の avg_val_r2 が最高の run を確認                │
│                          │                                      │
│  3. Kaggle Model としてアップロード                               │
│     src/kaggle_utils/customhub.py を使用                        │
│                          │                                      │
└──────────────────────────┼──────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────┐
│                        Kaggle 環境                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Input:                                                         │
│  ├── csiro-biomass/              # コンペデータ                  │
│  │   ├── test.csv                                               │
│  │   └── images/                                                │
│  └── csiro-biomass-artifacts/    # アップロードしたモデル         │
│      └── other/001/                                             │
│          └── run_0001/                                          │
│              └── fold_*/best_model.pth                          │
│                                                                 │
│  4. code.ipynb を実行                                            │
│     → inference.py の kaggle_inference() が呼ばれる             │
│     → submission.csv が生成される                                │
│                                                                 │
│  Output:                                                        │
│  └── submission.csv              # 提出ファイル                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 詳細手順

#### Step 1: トレーニング（ローカル）

```bash
cd experiments/001
python train.py --config configs/exp/exp001.yaml
```

#### Step 2: 結果確認

```bash
# summary.csv を確認してベストモデルを特定
cat output/{timestamp}_exp001/summary.csv
```

#### Step 3: モデルアップロード準備

`code.ipynb` の最後のセルをアンコメントして実行：

```python
if not config.IS_KAGGLE_ENV:
    from src.kaggle_utils.customhub import dataset_upload, model_upload

    # モデル重みを Kaggle Model としてアップロード
    model_upload(
        handle=config.ARTIFACTS_HANDLE,
        local_model_dir=config.OUTPUT_DIR,
        update=False,  # 更新時は True
    )

    # コードを Kaggle Dataset としてアップロード
    dataset_upload(
        handle=config.CODES_HANDLE,
        local_dataset_dir=config.ROOT_DIR,
        update=True,
    )
```

#### Step 4: Kaggle Notebook 作成

1. Kaggle で新しい Notebook を作成
2. Input に以下を追加：
   - コンペデータセット (`csiro-biomass`)
   - アップロードした Model (`csiro-biomass-artifacts`)
   - アップロードした Code Dataset (`csiro-biomass-codes`)
3. `code.ipynb` の内容を実行
4. Submit

#### Step 5: パラメータ調整

`code.ipynb` のセル2で推論パラメータを調整：

```python
RUN_ID = "run_0001"    # ベストモデルの run_id に変更
FOLDS = None           # None = 全5fold、[1,2,3] = 指定foldのみ
USE_SPLIT = False      # True で左右分割 TTA 有効
USE_TTA = True         # True で flip TTA 有効
USE_EMA = True         # True で EMA 重み使用
IMG_SIZE = 224         # トレーニング時と同じサイズ
```

---

## グリッドサーチの仕組み

### 動作原理

1. YAML ファイルをパース
2. リスト値を持つパラメータを抽出（NON_SWEEP_PATHS を除く）
3. すべての組み合わせを生成
4. 各組み合わせで独立したトレーニングを実行

### 例

```yaml
model:
  backbone:
    - tf_efficientnetv2_b0.in1k
    - tf_efficientnetv2_b1.in1k

optimization:
  lr:
    - 1e-3
    - 1e-4
```

この場合、以下の4つの run が生成されます：

| Run | backbone | lr |
|-----|----------|-----|
| 001 | efficientnetv2_b0 | 1e-3 |
| 002 | efficientnetv2_b0 | 1e-4 |
| 003 | efficientnetv2_b1 | 1e-3 |
| 004 | efficientnetv2_b1 | 1e-4 |

### run_id について

現在の実装では、トレーニング出力ディレクトリは以下の形式：

```
{timestamp}_{exp_name}/{index}_{backbone}__{params}/
```

Kaggle 提出時の `run_id` は、アップロード時のディレクトリ構造に依存します。

---

## 重要な実装の詳細

### 損失関数

```python
# Weighted Smooth L1 Loss
# 各ターゲットごとにバッチ平均のSmooth L1 Lossを計算し、重み付け合計

L_0 = mean(SmoothL1(pred[:, 0], target[:, 0]))  # Dry_Total_g
L_1 = mean(SmoothL1(pred[:, 1], target[:, 1]))  # GDM_g
L_2 = mean(SmoothL1(pred[:, 2], target[:, 2]))  # Dry_Green_g

loss = 0.5 * L_0 + 0.2 * L_1 + 0.1 * L_2

# SmoothL1 (beta=1.0):
# |x| < beta: 0.5 * x^2 / beta
# |x| >= beta: |x| - 0.5 * beta
```

### EMA (Exponential Moving Average)

```python
# モデル重みの指数移動平均を維持
# 学習の後半でより安定した重みを得られる

ema_weight = decay * ema_weight + (1 - decay) * model_weight

# デフォルト:
# - decay: 0.995
# - 開始: 総ステップの10%経過後
```

### TTA (Test Time Augmentation)

推論時に複数の変換を**分割前の元画像**に適用し、予測を平均：

```python
# TTA (3パターン) - 元画像に適用してから左右分割
transforms = [
    original,      # 変換なし
    hflip,         # 水平反転（左右が入れ替わる）
    vflip,         # 垂直反転
]

# 全 fold アンサンブル (例: 5fold × 3TTA = 15予測の平均)
```

### ターゲット導出

```python
def derive_all_targets(preds_3):
    """
    入力: [Dry_Total_g, GDM_g, Dry_Green_g]  # 予測値
    出力: [Dry_Green_g, Dry_Dead_g, Dry_Clover_g, GDM_g, Dry_Total_g]

    導出式:
    - Dry_Dead_g = max(Dry_Total_g - GDM_g, 0)
    - Dry_Clover_g = max(GDM_g - Dry_Green_g, 0)
    """
```

---

## トラブルシューティング

### よくあるエラー

#### 1. `FileNotFoundError: Train CSV not found`

```
原因: train_csv のパスが間違っている
解決: configs/exp/exp001.yaml の dataset.train_csv を確認
```

#### 2. `CUDA out of memory`

```
解決方法:
1. batch_size を減らす (32 → 16)
2. img_size を減らす (224 → 192)
3. 軽量なバックボーンを使用 (b1 → b0)
```

#### 3. `ModuleNotFoundError: No module named 'src'`

```
原因: 実行ディレクトリが experiments/001 ではない
解決: cd experiments/001 してから実行
```

#### 4. `KeyError: 'image_path'`

```
原因: train.csv のカラム名が想定と異なる
解決: src/data.py の image_col パラメータを確認
```

### デバッグ Tips

#### 1. 単一 fold で素早くテスト

```yaml
# configs/exp/exp001.yaml
trainer:
  fold: 0          # fold 0 のみ
  num_epochs: 2    # 2エポックのみ
```

#### 2. グリッドサーチを無効化

```yaml
# リストではなく単一値にする
model:
  backbone: tf_efficientnetv2_b0.in1k  # リストではない

optimization:
  lr: 1e-4  # リストではない
```

#### 3. ログの確認

```bash
# トレーニングログ
cat output/{timestamp}_exp001/001_xxx/logs/train.log

# fold 別メトリクス
cat output/{timestamp}_exp001/001_xxx/logs/metrics.csv
```

---

## 次のステップ（実験アイデア）

### 短期（精度向上）

1. **より大きなバックボーン**: `efficientnetv2_s`, `convnext_small`
2. **画像サイズ増加**: 224 → 384
3. **エポック数増加**: 10 → 30
4. **Augmentation 強化**: MixUp, CutMix

### 中期（アーキテクチャ）

1. **左右分割トレーニング**: `split_image: true`
2. **マルチスケール入力**: 複数解像度での学習
3. **損失関数の工夫**: Huber Loss, 組み合わせ損失

### 長期（アンサンブル）

1. **複数バックボーンの融合**: EfficientNet + ConvNeXt + Swin
2. **Pseudo Labeling**: テストデータの活用
3. **Stacking**: メタモデルによる統合

---

## 関連ドキュメント

- `docs/251209_compe_overview.md` - コンペ概要
- `docs/251209_data_explain.md` - データ形式の説明
- `docs/251209_baseline.md` - ベースライン戦略
- `docs/251209_implementation_plan.md` - 実装計画（詳細）
